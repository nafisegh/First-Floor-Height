{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596dfadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def modify_names(paths,suffix):\n",
    "\n",
    "    names=[os.path.basename(i) for i in glob.glob(paths)]\n",
    "\n",
    "\n",
    "    #names_=[j.replace('-',' ').split('_jpg',1)[0] for j in names]\n",
    "\n",
    "    #names_=[k.replace('cropped','').split('.jpg',1)[0] for k in names_]\n",
    "    \n",
    "    #names__=[kk.replace(' Copy','') for kk in names_]\n",
    "    \n",
    "    names__ = [j.replace('.jpg.jpg.jpg','.jpg').split('.jpg',1)[0] for j in names]\n",
    "    \n",
    "    for s,d in zip(names,names__):\n",
    "        \n",
    "        #if os.path.exists(paths.split('*',1)[0]+d) or not os.path.exists(paths.split('*',1)[0]+s):\n",
    "        \n",
    "        if os.path.exists('F:/Backup_Nafiseh_Hard_Drives/Medium FFH_new_2/dataset_folds_4/images/train/'+d+suffix):\n",
    "            \n",
    "           continue\n",
    "    \n",
    "        else:\n",
    "            \n",
    "            print(paths.split('*',1)[0]+s)\n",
    "            \n",
    "            #os.rename(paths.split('*',1)[0]+s,paths.split('*',1)[0]+d+suffix)\n",
    "    \n",
    "            os.rename(paths.split('*',1)[0]+s,'F:/Backup_Nafiseh_Hard_Drives/Medium FFH_new_2/dataset_folds_4/images/train/'+d+suffix)\n",
    "        \n",
    "            print(paths.split('*',1)[0]+d+suffix)\n",
    "\n",
    "\n",
    "modify_names('F:/Backup_Nafiseh_Hard_Drives/Medium FFH_new_2/dataset_folds_4/images/train/*.jpg','.jpg')\n",
    "\n",
    "#modify_names('//tsclient/C/Users/asus/Downloads/train_data_with_building_extent/combined_labels/*.txt','.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08428ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob\n",
    "\n",
    "\n",
    "\n",
    "#image_path='D:/GSV/TRCA_GTA_MOST_RECORDS/'\n",
    "\n",
    "image_path=r'F:\\Backup_Nafiseh_Hard_Drives\\High FFH_new\\*\\images\\train_0\\\\'\n",
    "\n",
    "#image_names=[os.path.basename(i) for i in glob.glob(image_path+'*.jpg')]\n",
    "\n",
    "image_paths=glob.glob(image_path+'*.jpg', recursive=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbebb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLD=5\n",
    "\n",
    "USE_FOLD=False\n",
    "\n",
    "SEED=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dfd818",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7235d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "df=pd.DataFrame(image_paths)\n",
    "\n",
    "fold=StratifiedKFold(n_splits=NUM_FOLD,shuffle=True,random_state=SEED)\n",
    "\n",
    "for n, (train_index,val_index) in enumerate(fold.split(df,np.ones((len(df),1)))):\n",
    "    \n",
    "    df.loc[val_index,'fold']=int(n)\n",
    "    \n",
    "df['fold']=df['fold'].astype('int')\n",
    "\n",
    "#df.to_csv('F:/Backup_Nafiseh_Hard_Drives/GSV/train_fold_Medium_FFH_2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc31f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('F:/Backup_Nafiseh_Hard_Drives/GSV/train_fold_Medium_FFH.csv')\n",
    "\n",
    "print(df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94694f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from shutil import copyfile, move, rmtree\n",
    "\n",
    "import os\n",
    "\n",
    "if USE_FOLD:\n",
    "    pass\n",
    "else:\n",
    "    # Remove existing dirs\n",
    "    for fold in range(NUM_FOLD):\n",
    "        # Prepare train and valid df\n",
    "        train_df = df.loc[df.fold != fold].reset_index(drop=True)\n",
    "        valid_df = df.loc[df.fold == fold].reset_index(drop=True)\n",
    "        \n",
    "        #try:\n",
    "        #rmtree(f'F:/Backup_Nafiseh_Hard_Drives/High FFH_new_2/dataset_folds_{fold}/images/train_0')\n",
    "        #rmtree(f'F:/Backup_Nafiseh_Hard_Drives/High FFH_new_2/dataset_folds_{fold}/labels/train_0')\n",
    "        #rmtree(f'F:/Backup_Nafiseh_Hard_Drives/Low FFH_new_2/dataset_folds_{fold}/images/valid')\n",
    "        #rmtree(f'F:/Backup_Nafiseh_Hard_Drives/Low FFH_new_2/dataset_folds_{fold}/labels/valid')\n",
    "        #except:\n",
    "            #print('No dirs')\n",
    "\n",
    "            # Make new dirs\n",
    "        os.makedirs(f'F:/Backup_Nafiseh_Hard_Drives/Medium FFH_new_2/dataset_folds_{fold}/images/train')\n",
    "        os.makedirs(f'F:/Backup_Nafiseh_Hard_Drives/Medium FFH_new_2/dataset_folds_{fold}/images/valid')\n",
    "        os.makedirs(f'F:/Backup_Nafiseh_Hard_Drives/Medium FFH_new_2/dataset_folds_{fold}/labels/train')\n",
    "        os.makedirs(f'F:/Backup_Nafiseh_Hard_Drives/Medium FFH_new_2/dataset_folds_{fold}/labels/valid')\n",
    "\n",
    "        # Move the images to relevant split folder.\n",
    "        for i in tqdm(range(len(train_df))):\n",
    "                row = train_df.loc[i]\n",
    "            \n",
    "                #print(row)\n",
    "                \n",
    "                try:\n",
    "\n",
    "                    copyfile(os.path.dirname(row.values[0].replace('images','labels'))+'\\\\'+os.path.basename(row.values[0]).split('.jpg',1)[0]+'.txt',\n",
    "                             f'F:/Backup_Nafiseh_Hard_Drives/Medium FFH_new_2/dataset_folds_{fold}/labels/train/'+os.path.basename(row.values[0]).split('.jpg',1)[0]+'.txt')\n",
    "\n",
    "                    copyfile(row.values[0], f'F:/Backup_Nafiseh_Hard_Drives/Medium FFH_new_2/dataset_folds_{fold}/images/train/{os.path.basename(row.values[0])}.jpg')\n",
    "\n",
    "                except FileNotFoundError:\n",
    "                    \n",
    "                    pass\n",
    "\n",
    "        for i in tqdm(range(len(valid_df))):\n",
    "            \n",
    "                 row = valid_df.loc[i]\n",
    "\n",
    "                 try:\n",
    "                 \n",
    "\n",
    "                    copyfile(os.path.dirname(row.values[0].replace('images','labels'))+'\\\\'+os.path.basename(row.values[0]).split('.jpg',1)[0]+'.txt',\n",
    "                            f'F:/Backup_Nafiseh_Hard_Drives/Medium FFH_new_2/dataset_folds_{fold}/labels/valid/'+os.path.basename(row.values[0]).split('.jpg',1)[0]+'.txt')\n",
    "                    \n",
    "                    \n",
    "                    copyfile(row.values[0], f'F:/Backup_Nafiseh_Hard_Drives/Medium FFH_new_2/dataset_folds_{fold}/images/valid/{os.path.basename(row.values[0])}.jpg')\n",
    "                    \n",
    "                 except FileNotFoundError:  \n",
    "                    \n",
    "                     pass\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa8f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global --unset https.proxy\n",
    "\n",
    "# Download YOLOv5\n",
    "!git clone https://github.com/ultralytics/yolov5  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf612783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('C:/users/nghasem2/yolov5/')\n",
    "# Install dependencies\n",
    "!pip install -qr requirements.txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3762a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "os.chdir('/')\n",
    "\n",
    "os.getcwd()\n",
    "\n",
    "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6219942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .yaml file \n",
    "import yaml\n",
    "\n",
    "for fold in range(NUM_FOLD):\n",
    "    data_yaml = dict(\n",
    "        train = f'//tsclient/F/cross_validation/dataset_folds_{fold}/images/train',\n",
    "        val = f'//tsclient/F/cross_validation/dataset_folds_{fold}/images/train',\n",
    "        nc = 4,\n",
    "        names = ['Building Extent', 'Front Door', 'Garage Door','Stairs'])\n",
    "\n",
    "    \n",
    "    with open(f'//tsclient/F/cross_validation/data_fold_{fold}.yaml', 'w') as outfile:\n",
    "        \n",
    "        yaml.dump(data_yaml, outfile, default_flow_style=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646c5c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('C:/users/nghasem2/yolov5/')\n",
    "\n",
    "IMG_SIZE=640 # Input image size.\n",
    "\n",
    "BATCH_SIZE=1 # Batch size\n",
    "\n",
    "EPOCHS=100# Number of epochs\n",
    "\n",
    "\n",
    "for fold in range(NUM_FOLD):  \n",
    "    \n",
    "    print('FOLD NUMBER: ', fold)\n",
    "    \n",
    "    !python train.py --img {IMG_SIZE} \\\n",
    "                     --batch {BATCH_SIZE} \\\n",
    "                     --epochs {1} \\\n",
    "                     --data //tsclient/F/cross_validation/data_fold_{fold}.yaml \\\n",
    "                     --weights yolov5s.pt \\\n",
    "                     --save-period 10\\\n",
    "                     --project yolov5-od-folds\\\n",
    "                     --name yolov5s-e-100-img-640-fold-{fold}\n",
    "    print('###########################################################################################\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad29dba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
